{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章 信息与熵\n",
    "\n",
    "## 7.1 度量信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(red ball)=4/9, information: 1.1699 bits\n",
      "P(green ball)=3/9, information: 1.585 bits\n",
      "P(yellow ball)=2/9, information: 2.1699 bits\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "I_red = -log2(4/9)\n",
    "I_green = -log2(3/9)\n",
    "I_yellow = -log2(2/9)\n",
    "print(f\"P(red ball)=4/9, information: {round(I_red, 4)} bits\")\n",
    "print(f\"P(green ball)=3/9, information: {round(I_green, 4)} bits\")\n",
    "print(f\"P(yellow ball)=2/9, information: {round(I_yellow, 4)} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 信息熵\n",
    "\n",
    "### 7.2.1 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 2.585\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# 概率分布\n",
    "p = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "# 计算熵\n",
    "e = entropy(p, base=2)\n",
    "print(f\"entropy: {e:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.3 相对熵和交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D(P||Q)=0.1231\n",
      "D(Q||P)=0.1406\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "p = [9/25, 12/25, 4/25]\n",
    "q = [1/3, 1/3, 1/3]\n",
    "\n",
    "d_pq = entropy(p, q, base=2)\n",
    "d_qp = entropy(q, p, base=2)\n",
    "\n",
    "print(f\"D(P||Q)={d_pq:.4f}\")\n",
    "print(f\"D(Q||P)={d_qp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [-2.2 -1.4 -0.8  0.2  0.4  0.8  1.2  2.2  2.9  4.6]\n",
      "y = [0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Q(y) = [0.19 0.33 0.47 0.7  0.74 0.81 0.86 0.94 0.97 0.99]\n",
      "Cross Entropy = 0.3329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])\n",
    "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "logr = LogisticRegression(solver='lbfgs')\n",
    "logr.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
    "loss = log_loss(y, y_pred)\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))\n",
    "print('Q(y) = {}'.format(np.round(y_pred, 2)))\n",
    "print('Cross Entropy = {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
